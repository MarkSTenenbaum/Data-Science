##______________________________________________##
#####   "Basic Statistics Part 1"
#
#### Thanks to Dr. David Gerard

##______________________________________________##

# Density curve------
# Let us find the area under the curve of function exp(-x^2)

## Normal Distribution------


x_vec <- seq(-5,5, length.out = 101)
f_vec <- exp(-x_vec^2)

# or by tidyverse
library(tidyverse)

df <- tibble(x_vec, f_vec)
df

df %>%
  ggplot(aes(x_vec, f_vec)) +
  geom_line() +
  labs(x = "x", y = "f(x)", title = "graph of exp(-x^2)")
  
library(pracma)         

f <- function(x){
  exp((-x^2)/2)
}

f(0)
 
a <- -Inf
b <- Inf

I <- integral(f, a, b)
I                          # Area under the entire curve  exp((-x^2)/2)
                              # is sqrt(2*pi)
near(I,sqrt(2*pi)) # 
# We want area under the curve exp(-x^2) be 1. (A standard normal distribution)
# 1/sqrt(pi) * exp(-x^2) and we call "standard normal distribution.

f1 <- function(x){
  exp((-x^2)/2)/sqrt(2*pi)
  }

 integral(f1, a, b)
 
# Let us define a function s.norm that will calculate 
      # area to the left of standard normal distribution.
      # It will use the function f1 that we created.
      # Also it compares the result of thr area to left 
      # that is generated by built in function pnorm()

s.norm <- function(z){
 
l <- length(z)
for (i in 1:l)
  {
  print(pnorm(z[i]))
 print(integral(f1, -Inf, z[i]))
  }
}


c <- c(-3:3)

s.norm(c) # it will calculate area to the left og z=-3, z=-2,..., z=3

### Distribution:------
# The possible values of a variable and how often it takes
# those values. 

### A density-------
# A density describes the distribution of a *quantitative* variable. 

# You can think of it as approximating a histogram. It is a curve where
### The area under the curve between any two points is approximately the
      # probability of being between those two points.
### The total area under the curve is 1 (something must happen).
### The curve is never negative (can't have negative probabilities).

### The distribution of many variables in Statistics approximate the 
#              **normal distribution**.
### If you know the mean and standard deviation of a normal distribution, then
#           you know the whole distribution.
### Larger standard deviation implies more spread out (larger and smaller values
 #                                                    are both more likely).
### Mean determines where the data are centered.

library(ggplot2)
library(ggthemes)
x <- seq(-10, 10, length = 100)
y1 <- dnorm(x = x, mean = 0, sd = 2)
y2 <- dnorm(x = x, mean = -4, sd = 2)
y3 <- dnorm(x = x, mean = 4, sd = 2)
dfdat <- data.frame(x = rep(x, 3), 
                    y = c(y1, y2, y3), 
                    z = factor(rep(c(1, 2, 3), each = length(x))))

ggplot(data = dfdat, mapping = aes(x = x, y = y, color = z, lty = z)) +
  geom_line(lwd=1) + 
  ggthemes::scale_color_colorblind() +
  theme(legend.position="none") +
  ylab("f(x)") +
  xlab("x")

### Normal densities with different standard deviations

x <- seq(-10, 10, length = 100)
y1 <- dnorm(x = x, mean = 0, sd = 1)
y2 <- dnorm(x = x, mean = 0, sd = 2)
y3 <- dnorm(x = x, mean = 0, sd = 4)
dfdat <- data.frame(x = rep(x, 3), 
                    y = c(y1, y2, y3), 
                    z = factor(rep(c(1, 2, 3), each = length(x))))

ggplot(data = dfdat, mapping = aes(x = x, y = y, color = z, lty = z)) +
  geom_line(lwd=1) + 
  ggthemes::scale_color_colorblind() +
  theme(legend.position="none") +
  ylab("f(x)") +
  xlab("x") 

### Density Function (height of curve, **NOT** probability of a value).

dnorm(x = 2, mean = 1, sd = 1)

# If you want to check. We can define the density curve for
# a normal distribution

f2 <- function(x, mu , sigma){
  exp((-(x-mu)^2)/(2*sigma^2))/(sigma*sqrt(2*pi))
}

f2(2, 1, 1)

x_vec <- seq(-5,5, length.out = 101)
f_vec_2 <- f2(x_vec, 1, 1)
f_vec_2



df <- tibble(x_vec, f_vec_2)
df

df %>%
  ggplot(aes(x_vec, f_vec_2)) +
  geom_line() +
  labs(x = "x", y = "f(x)", title = "graph of normal distribution")

f2(1, 1, 1) # the height must be at mu =1. because the curve has the pick there.
          # Also you mya find out by 1/(sigma*sqrt(2*pi))
1/(1*sqrt(2*pi))

### Random Generation------
# (generate samples from a given normal distribution).
samp <- rnorm(n = 1000, mean = 1, sd = 1)
head(samp)

# Quick Plot from {ggplot2}: 
#  is very similar to the basic plot() function from the R base package. 
# It can be used to create and combine easily different types of plots.
# However, it remains less flexible than the function ggplot().
qplot(samp)

qplot(samp, geom = "histogram", fill = I("white"), color = I("black"), bins = 20)


# or

samp %>% 
  qplot() +
  geom_histogram(fill = "white", color = "black", bins = 20)


# Or ggplot()

ggplot(samp, aes(x=samp)) + 
  geom_histogram(aes(y=..count..), colour="black", fill="white", bins = 20)
# We got error!


df_samp <- as.data.frame(samp)
ggplot(df_samp, aes(x=samp)) + 
  geom_histogram(aes(y=..count..), colour="black", fill="white", bins = 20)

# Cumulative Distribution Function (probability of being less than or equal 
#                                    to some value).

pnorm(q = 2, mean = 1, sd = 1)


x <- seq(-2, 4, length = 500)
y <- dnorm(x, mean = 1, sd = 1)
polydf <- data.frame(x = c(min(x), x[x < 2], 2, 0), 
                     y = c(0, y[x < 2], 0, 0))
qplot(x, y, geom = "line", ylab = "f(x)") +
  geom_polygon(data = polydf, mapping = aes(x = x, y = y), 
               fill = "blue", alpha = 1/4)

# or by ggplot
# This will produce a plot with a blue shaded region 
# and a black line representing the probability density 
# function of a normal distribution with 
# mean 1 and standard deviation 1. 
# The shaded region is the area under the curve for values
# of x between -2 and 2.

df <-  data.frame(x = x, y = y)
df %>%
  ggplot() +
  geom_line(mapping = aes(x = x, y = y),
            color = "black", size = 1) +
  geom_polygon(mapping = aes(x = x, y = y), data = polydf, 
               fill = "blue", alpha = 0.25) +
  ylab("f(x)") +
  xlim(-2, 4) +
  theme_classic()

### Quantile function------
#find value that has a given the probability of being less 
# than or equal to it.

qnorm(p = 0.8413, mean = 1, sd = 1)

x <- seq(-2, 4, length = 500)
y <- dnorm(x, mean = 1, sd = 1)
polydf <- data.frame(x = c(min(x), x[x < 2], 2, 0), 
                     y = c(0, y[x < 2], 0, 0))
qplot(x, y, geom = "line", ylab = "f(x)") +
  geom_polygon(data = polydf, mapping = aes(x = x, y = y), 
               fill = "blue", alpha = 1/4) +
  annotate(geom = "text", x = 0.5, y = 0.1, label = "0.8413", color = "black")


# By ggplot
x <- seq(-2, 4, length = 500)
y <- dnorm(x, mean = 1, sd = 1)
df <-  data.frame(x = x, y = y)
df %>%
  ggplot() +
  geom_line(mapping = aes(x = x, y = y),
            color = "black", size = 1) +
  geom_polygon(mapping = aes(x = x, y = y), data = polydf, 
               fill = "blue", alpha = 0.25) +
  ylab("f(x)") +
  xlim(-2, 4) +
  annotate(geom = "text", x = 0.5, y = 0.1, label = "0.8413", color = "black") +
  theme_classic()
## T Distribution-------
# The $t$-distribution shows up a lot in Statistics. 
# It is also bell-curved but has "thicker tails" (more extreme observations 
                                                  are more likely). 
## It is always centered at 0. 
## It only has one parameter, called the "degrees of freedom", which 
     # determines how thick the tails are.
## Smaller degrees of freedom mean thicker tails, larger degrees of freedom
     # means thinner tails.
## If the degrees of freedom is large enough, the $t$-distribution
     # is approximately the same as a normal distribution with mean 0
     # and variance 1.

## $t$-distributions with different degrees of freedom:
  

x <- seq(-4, 4, length = 200)
data.frame(df = as.factor(c(rep(1, length(x)), rep(5, length(x)), rep(Inf, length(x)))),
           x = c(x, x, x),
           y = c(dt(x = x, df = 1),
                 dt(x = x, df = 5),
                 dt(x = x, df = Inf))) ->
  dfdat
ggplot(dfdat, mapping = aes(x = x, y = y, color = df, lty = df)) +
  geom_line() +
  scale_color_colorblind() +
  ylab("f(x)")

### Density Function------------
dt(x = -6, df = 2)

x <- seq(-6, 6, length = 100)
y <- dt(x, df = 1)
qplot(x, y, geom = "line", ylab = "f(x)") +
  geom_segment(dat = data.frame(x = 2, xend = 2, y = 0,
                                yend = dt(x = 2, df = 2)), 
               aes(x = x, y = y, xend = xend, yend = yend), lty = 2, col = 2)
### Random Generation------
samp <- rt(n = 1000, df = 2)
head(samp)

qplot(samp, geom = "histogram", 
      fill = I("white"), color = I("black"),
      bins = 20)
### Cumulative Distribution Function
pt(q = 2, df = 2)

x <- seq(-6, 6, length = 500)
y <- dt(x,  df = 2)
polydf <- data.frame(x = c(min(x), x[x < 2], 2, 0), 
                     y = c(0, y[x < 2], 0, 0))
qplot(x, y, geom = "line", ylab = "f(x)") +
  geom_polygon(data = polydf, mapping = aes(x = x, y = y), 
               fill = "blue", alpha = 1/4)

### Quantile Function-------
qt(p = 0.9082, df = 2)

x <- seq(-6, 6, length = 500)
y <- dt(x, df = 2)
polydf <- data.frame(x = c(min(x), x[x < 2], 2, 0), 
                     y = c(0, y[x < 2], 0, 0))
qplot(x, y, geom = "line", ylab = "f(x)") +
  geom_polygon(data = polydf, mapping = aes(x = x, y = y), 
               fill = "blue", alpha = 1/4) +
  annotate(geom = "text", x = 0, y = 0.1, label = "0.9082", color = "black")

# More Distribution:------
# There are many other distributions implemented in R. To see the most common,
#   run:

help("Distributions")


# Observational/experimental Units-------

# people/places/things/animals/groups
# that we collect information about. Also known as "individuals" or "cases".
# Sometimes I just say "units".

#### **Variable**: A property of the observational/experimental units. 
     # E.g.: height of a person, area of a country, marital status.

#### **Value**: The specific level of a variable for an observational/experimental 
   # unit.
     # E.g.: Bob is 5'11'', China has an area of 3,705,407 square miles, Jane is divorced.
    
#### **Quantitative Variable**: The variable takes on numerical values where arithmetic 
#     operations (plus/minus/divide/times) make sense. 
      # E.g.: height, weight, area, income.
      # Counterexample: Phone numbers, social security numbers.
    
#### **Categorical Variable**: The variable puts observational/experimental units 
     #  into different groups/categories based on the values of that variable.
      #  E.g.: race/ethnicity, marital status, religion.

#### **Binary Variable**: A categorical variable that takes on only two values.
     # E.g.: dead/alive, treatment/control.
    
#### **Population**: The collection of all observational units we are interested in. 

#### **Parameter**: A numerical summary of the population.
     # E.g.: Average height, proportion of people who are divorced, standard
     #  deviation of weight.

#### **Sample**: A subset of the population (some observational units, but 
     # not all of them).

#### **Statistic**: A numeric summary of the sample.
    #   E.g.: Average height of the sample, proportion of people who are 
      # divorced in the sample, standard deviation of weight of a sample.

#### **Sampling Distribution**: The distribution of a statistic over
    # many hypothetical random samples from the population.

dnorm(x = 10, mean = 10, sd = 2)


##______________________________________________##
#####   "Basic Statistics Part 2"
#
#### Thanks to Dr. David Gerard

##______________________________________________##

# All Of Statistics-------------
## Observational/experimental Units-------

# people/places/things/animals/groups
# that we collect information about. Also known as "individuals" or "cases".
# Sometimes I just say "units".

#### **Variable**: A property of the observational/experimental units. 
# E.g.: height of a person, area of a country, marital status.

#### **Value**: The specific level of a variable for an observational/experimental 
# unit.
# E.g.: Bob is 5'11'', China has an area of 3,705,407 square miles, Jane is divorced.

#### **Quantitative Variable**: The variable takes on numerical values where arithmetic 
#     operations (plus/minus/divide/times) make sense. 
# E.g.: height, weight, area, income.
# Counterexample: Phone numbers, social security numbers.

#### **Categorical Variable**: The variable puts observational/experimental units 
#  into different groups/categories based on the values of that variable.
#  E.g.: race/ethnicity, marital status, religion.

#### **Binary Variable**: A categorical variable that takes on only two values.
# E.g.: dead/alive, treatment/control.

#### **Population**: The collection of all observational units we are interested in. 

#### **Parameter**: A numerical summary of the population.
# E.g.: Average height, proportion of people who are divorced, standard
#  deviation of weight.

#### **Sample**: A subset of the population (some observational units, but 
# not all of them).

#### **Statistic**: A numeric summary of the sample.
#   E.g.: Average height of the sample, proportion of people who are 
# divorced in the sample, standard deviation of weight of a sample.

#### **Sampling Distribution**: The distribution of a statistic over
# many hypothetical random samples from the population.

# All of Statistics: We see a pattern in the sample.
## **Estimation**:--------
##### Guess the pattern in the population based on the sample. 
##### Guess a parameter with a statistic. A statistic which is a guess for a 
#   parameter is called an **estimate**.


##- **Hypothesis Testing**:----------------
#### Ask if the pattern we see in the sample also
#### exists in the population. Test if a parameter is some value.


## **Confidence Intervals**:----------------
#### Quantify our (un)certainty of the pattern 
#### in the population based on the sample. Provide a range of likely 
####parameter values.

###### We will go through a lot of examples of this below    

###### For the examples below, we will use the data from the Sleuth3 package in R.

library(Sleuth3)
library(tidyverse)
library(broom)

help(ex0223) # Read help page and find

# What are the observational units? 
# What are the variables? Which
# are quantitative and which are categorical?

glimpse(ex0223)

### SOLUTION:
# What are the observational units? 
# Observational Units: The states (and DC).

# What are the variables? 
# Variables: `State` (primary key), `Fatalities1995`,
#`Fatalities1996`, `PctChange`, `SpeedLimit`.

# Which are quantitative and which are categorical?
# Variables: `State` (primary key), `Fatalities1995` (quantitative),
#`Fatalities1996` (quantitative), `PctChange` (quantitative),
#`SpeedLimit` (categorical).




## Pattern: Mean is shifted (one quantitative variable)----
glimpse(case0202)
### **Example**:--------
# Researchers measured the volume of the left hippocampus in 15 twins
# where one twin had schizophrenia and the other did not. They were interested
# in whether the left hippocampus differed in size between the normal and 
# schizophrenic twin.

#### Observational Units: The twins.

#### Population: All twins where one has schizophrenia and the other 
# does not.

#### Sample: The 15 twins in our study.

#### Variable: The difference in volume in the left hippocampus between the twins.

#### We derived this quantitative variable by subtracting one volume from another.

case0202 %>%
  as_tibble() %>%
  mutate(diff = Unaffected - Affected) %>%
  select(diff) ->
  schizo
glimpse(schizo)

## Pattern: Use a histogram/boxplot to visualize the shift from 0.-----


ggplot(schizo, aes(x = diff)) +
  geom_histogram(bins = 15, fill = "white", color = "black") +
  geom_vline(xintercept = 0, lty = 2) +
  xlab("Difference in Brain Volumes")



### Parameter of interest:------
# Mean difference in left hippocampus volumes for all twins.

### Estimate: Use sample mean-----


schizo %>%
  summarize(meandiff = mean(diff))


### 0.199 is our "best guess" for the parameter, but it is almost certainly not
### the value of the parameter (since we didn't measure everyone).

## Hypothesis Testing:----------------- 
#### We are interested in if the mean difference is different from 0. 
#### Two possibilities:
##  1. **Alternative Hypothesis**: Mean is different from 0.
##  2. **Null Hypothesis**: Mean is not different from 0, we just happened
#   *by chance* to get twins that had a big difference in volume.

#### Strategy: We calculate the probability of the data assuming possibility 2 
# (called a $p$-value). If this probability is low, we conclude possibility 1. 
# If the this probability is high, we don't conclude anything.

#### **p-value**:-------
# the probability that you would see data as or more 
# supportive of the alternative hypothesis than what you saw 
# *assuming that the null hypothesis is true*.



###  The distribution of possible null sample means is given by statistical theory.
# Specifically, the $t$-statistic (mean divided by the standard deviation of the
#   sampling distribution of the mean) has a $t$ distribution with
# $n - 1$ degrees of freedom ($n$ is the sample size). 
# It works as long as your data aren't too skewed or if you have a large 
# enough sample size.

###  Function: `t.test()`-------------------


tout <- t.test(schizo$diff)
tout


#### The `tidy()` function---------
# tidy() from the broom package will format the output of common
# procedures to a convenient data frame.


tdf <- tidy(tout)
tdf$estimate
tdf$p.value

## confidence intervals----------------------
#### We often want a range of "likely" values. These are called confidence
####intervals. `t.test()` will return these confidence intervals, giving
#### lowest and highest likely values for the mean difference in volumes:

tdf$conf.low
tdf$conf.high
s_diff <- schizo$diff

n <- length(s_diff)
n

# There is no function for confidence interval in {base} R
# We know (xbar - margin, xbar + margin)

### Sample mean
xbar <- mean(s_diff) # it also can be obtained by `tdf$estimate`. Question what was tdf? Scroll up
xbar

### Need to find margin of error for 95% confidence
##  z_{0.95}*s_dev / sqrt(n)

#### z_0 or  z_{0.95} 
# Note n=15 and z = 0.95 + (1-0.95)/2 =0.975
z <- 0.975
z_0 <- qt(z,df=n-1)
z_0
### Sample standard deviation
s_dev <- sd(s_diff) 
s_dev
# or sqrt(var(s_diff))
sqrt(var(s_diff))

margin <- z_0 *s_dev/sqrt(n)

margin


lower_confid <- xbar - margin
lower_confid
# Compare it with tdf$conf.low
tdf$conf.low

upper_confid <- xbar + margin
upper_confid
# Compare it with tdf$conf.hight
tdf$conf.high


### Interpreting confidence intervals:-----------------
##### CORRECT: We used a procedure that would capture the true parameter in 95%
# of repeated samples.
##### CORRECT: *Prior to sampling*, the probability of capturing the true
# parameter is 0.95.

##### WRONG: After sampling, the probability of capturing the true parameter
# is 0.95. 
### Because after sampling the parameter is either in the interval
# or it's not. We just don't know which.

#### WRONG: 95% of twins have volume differences within the bounds of the 95% 
# confidence interval. 
### Because confidence intervals are statements about
# parameters, not observational units or statistics.

##### Graphic:----------------

library(ggthemes)
set.seed(1)
ncoll <- 20
nind <- 15
meanval <- 0.25
sig <- 0.23
tibble(diff2 = rnorm(n = nind * ncoll, mean = meanval, sd = sig),
       sample = rep(1:nind, each = ncoll)) %>%
  group_by(sample) %>%
  nest() %>%
  mutate(ttest = map(data, ~t.test(.$diff2)),
         ttest = map(ttest, tidy)) %>%
  unnest(ttest, .drop = TRUE) %>%
  select(sample, conf.low, conf.high) %>%
  mutate(cover = factor(conf.low < meanval & conf.high > meanval, levels = c("TRUE", "FALSE"))) %>%
  ggplot(aes(x = sample, xend = sample, y = conf.low, yend = conf.high, color = cover)) +
  geom_segment(lwd = 2) +
  geom_hline(yintercept = meanval, lty = 2) +
  theme_classic() +
  theme(legend.position = "none") +
  xlab("Sample Number") +
  ylab("Volume Difference") +
  scale_x_continuous(breaks = 1:ncoll) +
  scale_color_colorblind() ->
  pl
pl



#### Intuition:---------
# Statistical theory tells us that the sample mean will be
# within (approximately) 2 standard deviations of the population 
# mean in 95% of repeated samples. This is two standard deviations
# of the sampling distribution of the sample mean, *not* two standard
# deviations of the sample. So we just add and subtract (approximately)
# two standard deviations of the sampling distribution from the sample mean.




# Pattern: Means of two groups are different (one quantitative, one binary)-----

### **Example**:-------- 
# Beaks depths were measured on Daphne Major finches in 1976 (before
# a harsh drought) and in 1978 (after a harsh drought). The researchers
# hypothesized that finches with deeper peaks were more likely to survive.


data("case0201")
case0201 %>%
  mutate(Year = as.factor(Year)) ->
  finch
glimpse(finch)


### Observational Units: The finches.

### Population: All finches.

### Sample: The 178 finches that the researches measured.

### Variables: The year the finch was measured (binary/categorical) and the
# beak depth (quantitative). Possible to also treat year as quantitative.

### Pattern: Use a boxplot to see if the groups differ.-----


ggplot(finch, aes(x = Year, y = Depth)) +
  geom_boxplot(coef = Inf) +
  geom_jitter(alpha = 1/3)


###  Parameter of interest: Difference in mean beak depths between 1976 finches
# and 1978 finches.

### Estimate: The difference in mean beak depths between 1976 finches and 
# 1978 finches in our sample.


finch %>%
  group_by(Year) %>%
  summarize(meandepth = mean(Depth)) %>%
  spread(key = Year, value = meandepth) %>%
  mutate(diff = `1978` - `1976`) ## do you remember why we need back-ticks?



## Hypothesis Test:---------
### We want to know if the difference in the mean depths in the two years is
#    actually different.

###  Two possibilities:-----------

#### **Alternative Hypothesis**:-------------
# The mean depths are different in the two years.
#### **Null Hypothesis**:-------------
# The mean depths are the same in the two years. We
# just happened by chance to get deep 1978 finches and shallow 1976 finches.

### Strategy: We calculate the probability of the data assuming possibility 2
# (called a p-value). If this probability is low, we conclude possibility 1.
# If the this probability is high, we donâ€™t conclude anything.










